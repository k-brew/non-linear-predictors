---
title: "Project 003 - Nonlinear Predictors"
author: "Kyle Brewster"
date: "3/13/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
(.packages())
```

# Part 0: Prep Work

Loading packages

```{r run_4fresh_start, message=FALSE, warning=FALSE, results='hide'}
library(pacman)
p_load(readr,      # Reading csv
       dplyr,      # Syntax
       magrittr,   # Piping
       tidymodels, # Modeling
       rpart.plot,  # Plotting
       baguette,   # Bagging trees
       randomForest,      # Random forests
       caret,      # General model fitting
       rpart,
       parsnip,
       ipred)

election = read_csv("election_2016.csv")

## Cleaning

# Adding a variable found to be significant from last time
election %<>%
  mutate(log_inc_hh = log(income_median_hh),
         log_home_med = log(home_median_value),
         intrxt_var = (log_inc_hh*log_home_med),
         n_dem_var = (n_votes_democrat_2012/n_votes_total_2012),
         n_rep_var = (n_votes_republican_2012/n_votes_total_2012),
         i_republican_2012 = if_else(i_republican_2012==1,
                                     "Rep_maj", "Dem_maj"),
         i_republican_2016 = if_else(i_republican_2016==1, 
                                     "Rep_maj", "Dem_maj"),
         state = usdata::state2abbr(election$state))
# Last line to help save space for plotting trees

set.seed(123)
# Creating Train/Test Splits
train_elect = election %>%  sample_frac(0.8)
test_elect = anti_join(election, train_elect, by = 'fips')

# Removing 'fips' since it is an indicator value
train_elect %<>%  select(-c('fips'))
test_elect %<>% select(-c('fips'))
election %<>% select(-c('fips'))

# Duplicating for consequence-free sandboxing and removing county for better results
train_1 <- train_elect %>% select(-c("county"))
test_1 <- test_elect %>% select(-c("county"))
```

# Individual Decision Trees

```{r one_tree, message=FALSE, warning=FALSE}
default_cv = train_1 %>% vfold_cv(v =5)

# Define the decision tree
default_tree = decision_tree(mode ="classification",
                             cost_complexity = tune(),
                             tree_depth = tune()) %>%
               set_engine("rpart")
               
# Defining recipe
default_recipe = recipe(i_republican_2016 ~., data = train_1)

# Defining workflow
default_flow = workflow() %>%
  add_model(default_tree) %>%
  add_recipe(default_recipe)

# Tuning
default_cv_fit = default_flow %>%
  tune_grid(
    default_cv,
    grid = expand_grid(
      cost_complexity = seq(0, 0.15, by = 0.01),
      tree_depth = c(1,2,5,10),
    ),
    metrics = metric_set(accuracy, roc_auc)
  )

# Fitting the best model
best_flow = default_flow %>%
  finalize_workflow(select_best(default_cv_fit, metric = "accuracy")) %>%
  fit(data = train_1)

# Choosing the best model
best_tree = best_flow %>% extract_fit_parsnip()

# Plotting the tree
best_tree$fit %>% rpart.plot::rpart.plot(roundint=F)

# Printing summary statistics
printcp(best_tree$fit)
best_tree$fit$variable.importance

# Creating new df to hold predicted values for later comparison
comp_df = train_1 %>% select(c(i_republican_2016))

comp_df$one_tree_1 = predict(best_tree, new_data=train_1)
```

```{r two_tree, message=FALSE, warning=FALSE}
# Defining another tree with tuning adjustments
default_tree2 = decision_tree(mode ="classification",
                             cost_complexity = 0.005,
                             tree_depth = 10) %>%
               set_engine("rpart")

# Defining recipe
default_recipe = recipe(i_republican_2016 ~., data = train_1)

# Defining workflow
default_flow = workflow() %>%
  add_model(default_tree2) %>%
  add_recipe(default_recipe)

# Tuning
default_cv_fit = default_flow %>%
  tune_grid(
    default_cv,
    grid = expand_grid(
      cost_complexity = seq(0, 0.15, by = 0.01),
      tree_depth = c(1,2,5,10),
    ),
    metrics = metric_set(accuracy, roc_auc)
  )

# Fitting the best model
best_flow = default_flow %>%
  finalize_workflow(select_best(default_cv_fit, metric = "accuracy")) %>%
  fit(data = train_1)

# Choosing the best model
best_tree = best_flow %>% extract_fit_parsnip()

# Plotting the tree
best_tree$fit %>% rpart.plot::rpart.plot(roundint=F)

# Printing summary statistics
printcp(best_tree$fit)
best_tree$fit$variable.importance

# Adding prediction to comparison data frame
comp_df$one_tree_2 = predict(best_tree, new_data=train_1)

```

```{r three_tree, message=FALSE, warning=FALSE}
# And another with different tuning
default_tree3 = decision_tree(mode ="classification",
                             cost_complexity = 0.05,
                             tree_depth = 5) %>%
               set_engine("rpart")

# Defining recipe
default_recipe = recipe(i_republican_2016 ~., data = train_1)

# Defining workflow
default_flow = workflow() %>%
  add_model(default_tree3) %>%
  add_recipe(default_recipe)

# Tuning
default_cv_fit = default_flow %>%
  tune_grid(
    default_cv,
    grid = expand_grid(
      cost_complexity = seq(0, 0.15, by = 0.01),
      tree_depth = c(1,2,5,10),
    ),
    metrics = metric_set(accuracy, roc_auc)
  )

# Fitting the best model
best_flow = default_flow %>%
  finalize_workflow(select_best(default_cv_fit, metric = "accuracy")) %>%
  fit(data = train_1)

# Choosing the best model
best_tree = best_flow %>% extract_fit_parsnip()

# Plotting the tree
best_tree$fit %>% rpart.plot::rpart.plot(roundint=F)

# Printing summary statistics
printcp(best_tree$fit)
best_tree$fit$variable.importance

comp_df$one_tree_3 = predict(best_tree, new_data=train_1)

```

# Part 2: Bagging

```{r}
# Define the decision tree
default_treebag = bag_tree(mode ="classification",
                             cost_complexity = 0,
                              min_n = 2) %>%
               set_engine(engine = "rpart", times = 10)

# Defining workflow
default_flow = workflow() %>%
  add_model(default_treebag) %>%
  add_recipe(default_recipe)

fitt = default_flow %>% fit(train_1)

fitt

comp_df$pred_bag = predict(fitt, new_data=train_1)

# out-of-bag estimate
mean(predict(fitt, new_data=train_1) != train_1$i_republican_2016)
```

# Part 3: Forests

```{r web_ex}
train_1 %<>%
   mutate(i_republican_2016 = as.factor(i_republican_2016),
          i_republican_2012 = as.factor(i_republican_2012),
          state = as.factor(state))

class_rf =  randomForest(formula = i_republican_2016 ~ .,
                        data = train_1,
                        importance = TRUE,
                        ntree = 50)
importance(class_rf)
comp_df$pred_rf = predict(class_rf, type="response", newdata = train_1)

confusion_mtrx = table(train_1$i_republican_2016, comp_df$pred_rf)
confusion_mtrx # Printing confusion matrix 
```

I had originally set `n = 50` so that I could get the model to function properly and had planned to increase the value once I was confident in the functionality of the code, but turns out that 50 was a good value and resulted in great model performance.


# Part 4: Boosting

```{r running}
default_boost = boost_tree(mode ="classification",
                           engine = "xgboost")

predy = fit(default_boost, formula = i_republican_2016~.,control = control_parsnip(), data = train_1)

comp_df$pred_boost = predict(predy, new_data=train_1)

vec = comp_df %>% pull(pred_boost) %>% as.data.frame(.)
vec2=as_tibble(comp_df$i_republican_2016) %>% as.data.frame(.)

df = vec %>%
   select(.pred_class)%>%mutate(predss = vec$.pred_class) %>%
   mutate(predss = as.character(predss),
          og_val = if_else(
             vec2$value=="Rep_maj","Rep_Maj","Dem_maj"))

confusion_mtrx2 = table(df$predss,df$og_val)
confusion_mtrx2
predy
```

# Part 5: Reflection

All of the models above suggested that certain variables we more explanatory than others for predicting the outcome variable `i_republican_2016`

Looking at modeling using a single decision tree, we can see the variation that can arise from tuning the hyperparameters. Each of the individually planted trees had a root node error of 0.15483. This means that these models were incorrect at assigning a given observation to the correct path/spit at the first splitting node. While the end of a split might still result in the correct prediction, that is because we attempting to predict a binary variable; an incorrect assignment at the first split when attempting to predict an outcome that is continuous or with multiple levels. In such cases, more information will be lost with an inaccurate initial assessment.

The last single decision tree that was plotted above provides a fitting visual for this concern from single tree modeling. Since the variable of greatest importance is `n_dem_var` in all of the models, the first split of the tree will be the same for all correct and incorrect assignments.

The out-of-bag error rate estimate for bagging model was 0.00080, which suggests that this model performs well at predicting the outcome variable.

I found it intesting that the `state` variable was not among the top-ranked variables in terms of importance for the single decision tree models, but was ranked as the highest variable for the random forest modeling. When looking at the results of `importance(class_rf)` in part 3 of the code above, we can see that the mean decrease in accuracy is high for the `state` variable as well as many of the other variables that were also considered important in earlier models. A higher value tells us the degree to which the model will loose accuracy if the given variable is excluded from modeling.

Similarily, we can see high values of the mean decrease in Gini coefficient for the variables that this model selected as important. This value provides a measurements for the degree to which each variable contributes to homogeneity of a region (i.e. its purity). If a region is very homogeneous, then the Gini index will be small. In this presentation of summarizing statistics, a lower Gini is represented by a higher decrease in mean Gini, meaning that the given predictor variables plays a greater role in separating the data into the classes defined in the model.

Looking at the confusion matrix for the predicted values of the random forest, it was able to achieve 100% accuracy from the given data. The boosted tree model performed not quite as well, but had strong accuracy in predicting values nonetheless. 

```{r}
# Equalizing classes of train and test set
xtest <- rbind(train_1[1, ] , train_1)
xtest <- xtest[-1,]

# Predicting on testing set with model believed to be strongest
xtest$p = predict(class_rf, newdata = xtest, type = "class")

# Cleaning for matri
df = xtest %>%
   mutate(p = as.character(p),
          i_republican_2016 = as.character(i_republican_2016)) 
          
# Confusion Matrix
table(df$i_republican_2016, df$p)
```



# Part 6: Review

### 14. Why are boosted-tree ensembles so sensitive to the number of trees (relative to the bagged-tree ensembles and random forests)?

Boosted-tree ensembles are more sensitive to the number of trees compared to bagging or random forests because boosting allows trees to pass information to other trees. Since trees in boosting are trained on residual values from previous trees during the modeling process. 

### 15. How do individual decision trees guard against overfitting?

One way you can guard against over fitting with individual trees is by tuning the number of splits. A higher number of splits for the final selected modeling may result in better model performance for the initial data set, but would become less flexible when using on other data and can result in less interpretability.

We can address these issues by pruning our selected trees. If a variation of the modeling increases variance at a higher rate than it reduces bias (i.e. the bias-variance trade off), then pruning to remove those regions can improve performance in terms of testing MSE. 

### 16. How do ensembles trade between bias and variance?

For ensemble methods, an estimators variance typically decreases as the selected sampling size increases. With this in mind, including a higher number of trees when bagging or growing forests will result in individual trees that are very flexible and noisy, but an aggregate that stabilizes.

### 17. How do trees allow interactions?

Utilizing methods involving decision trees in prediction allows for models to consider interaction that may be occurring between variables that is much more difficult to capture with a simple linear model. It might be possible to use a simple regression model to fit the training data, but it will likely be overfitting and have poor performance during testing or with new data (or might suggest that perhaps decision trees aren't going to be the best option for modeling a trend).

As a result, trees are able to replicate nonlinear boundaries in data better than other methods and are simple to explain, interpret, and provide graphical visualizations to describe the model.


